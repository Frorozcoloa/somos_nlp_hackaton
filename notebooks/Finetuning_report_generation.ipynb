{"cells":[{"cell_type":"markdown","id":"202119b5-da5a-4b5b-a63c-093435660519","metadata":{"id":"202119b5-da5a-4b5b-a63c-093435660519"},"source":["### Instalación de librerías básicas"]},{"cell_type":"code","execution_count":null,"id":"a01287d8-3f62-4847-823c-c4027ff91aa5","metadata":{"id":"a01287d8-3f62-4847-823c-c4027ff91aa5"},"outputs":[],"source":["!pip install transformers argilla datasets"]},{"cell_type":"code","execution_count":null,"id":"622692cb-359c-4303-ba1f-2add1a75c160","metadata":{"id":"622692cb-359c-4303-ba1f-2add1a75c160"},"outputs":[],"source":["!pip uninstall -y huggingface_hub && pip3 install git+https://github.com/huggingface/huggingface_hub"]},{"cell_type":"code","execution_count":null,"id":"db120081-3291-4e6d-a342-521536b7aaf9","metadata":{"id":"db120081-3291-4e6d-a342-521536b7aaf9"},"outputs":[],"source":["!pip install -q bitsandbytes accelerate loralib\n","!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"]},{"cell_type":"markdown","id":"acfecc80-1e2a-4e67-a218-3ca73bd75d37","metadata":{"id":"acfecc80-1e2a-4e67-a218-3ca73bd75d37"},"source":["### Fine-tuning a Bertin usando LoRA"]},{"cell_type":"code","execution_count":null,"id":"8236c89c-252a-4b78-8106-527208febba4","metadata":{"id":"8236c89c-252a-4b78-8106-527208febba4"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"id":"5096c2ce-1f8c-4d24-92bd-e3d791211a74","metadata":{"id":"5096c2ce-1f8c-4d24-92bd-e3d791211a74"},"outputs":[],"source":["import torch\n","import requests\n","from transformers import BertTokenizerFast, EncoderDecoderModel, AutoTokenizer, AutoConfig, AutoModelForCausalLM\n","from datasets import DatasetDict, Dataset, load_dataset\n","import argilla as rg\n","import pandas as pd\n","import transformers\n","import os\n","import torch.nn as nn\n","import bitsandbytes as bnb\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""]},{"cell_type":"code","execution_count":null,"id":"7464cd30-7540-4a8f-9a0f-371bfdab0683","metadata":{"id":"7464cd30-7540-4a8f-9a0f-371bfdab0683"},"outputs":[],"source":["model_id = 'bertin-project/bertin-gpt-j-6B'"]},{"cell_type":"code","execution_count":null,"id":"f7028fba-e7a9-4ff8-aef2-5bc616299660","metadata":{"id":"f7028fba-e7a9-4ff8-aef2-5bc616299660"},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(model_id,\n","                                             load_in_8bit=True,\n","                                             device_map='auto',\n","                                             torch_dtype=torch.float16)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":null,"id":"6df1f309-1700-4e8c-bf75-b9db2c1d642b","metadata":{"id":"6df1f309-1700-4e8c-bf75-b9db2c1d642b","outputId":"fa3c0727-7bb4-4bba-b5e5-c5072601bbb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Whe have added 2 tokens\n"]},{"data":{"text/plain":["Embedding(50259, 4096)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.pad_token = tokenizer.eos_token\n","\n","new_tokens = [\"<SH>\", \"<EH>\"]\n","num_added_toks = tokenizer.add_tokens(new_tokens)\n","print(f\"Whe have added {num_added_toks} tokens\")\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","id":"0b752b63-5cac-4154-8c3c-779d610deb8e","metadata":{"id":"0b752b63-5cac-4154-8c3c-779d610deb8e"},"source":["### Postprocesado del modelo"]},{"cell_type":"code","execution_count":null,"id":"f152e0e1-333a-4473-9fc9-2d7d692bb6c6","metadata":{"id":"f152e0e1-333a-4473-9fc9-2d7d692bb6c6"},"outputs":[],"source":["for param in model.parameters():\n","  param.requires_grad = False  # freeze the model - train adapters later\n","  if param.ndim == 1:\n","    # cast the small parameters (e.g. layernorm) to fp32 for stability\n","    param.data = param.data.to(torch.float32)\n","\n","model.gradient_checkpointing_enable()  # reduce number of stored activations\n","model.enable_input_require_grads()\n","\n","class CastOutputToFloat(nn.Sequential):\n","  def forward(self, x): return super().forward(x).to(torch.float32)\n","model.lm_head = CastOutputToFloat(model.lm_head)"]},{"cell_type":"markdown","id":"78c58f69-5cd0-4f58-9848-7c1474656847","metadata":{"id":"78c58f69-5cd0-4f58-9848-7c1474656847"},"source":["\n","### Aplicar LoRA\n"]},{"cell_type":"code","execution_count":null,"id":"d15b0d65-2aa1-419f-816d-159a7671b34e","metadata":{"id":"d15b0d65-2aa1-419f-816d-159a7671b34e"},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n"]},{"cell_type":"code","execution_count":null,"id":"734b6c5a-c14d-41ae-a4c6-5a39c9bedda8","metadata":{"id":"734b6c5a-c14d-41ae-a4c6-5a39c9bedda8","outputId":"d0ec8034-6f3c-41d1-9388-773088640592"},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 7340032 || all params: 6057067603 || trainable%: 0.12118127914511903\n"]}],"source":["from peft import LoraConfig, get_peft_model \n","\n","config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","model = get_peft_model(model, config)\n","print_trainable_parameters(model)"]},{"cell_type":"markdown","id":"0d7aeb98-6a01-4e3b-8d57-47aa6c36e256","metadata":{"id":"0d7aeb98-6a01-4e3b-8d57-47aa6c36e256"},"source":["### Preprocesado del dataset"]},{"cell_type":"code","execution_count":null,"id":"45e1cbef-eb0d-4d0b-8ab3-526a66fdee7c","metadata":{"colab":{"referenced_widgets":["8f0abf80f97842068e51b8792f1ebdd1","17aa984267e446fa92d50ca1a89f59e0","df0e4ef3e4b54610bf7c160c412c72a2","8f51ee9db3c143f1965d14ff63587cf9","0929705b82034919bfb3d47ca10c6de7","6d04076d110e45d9abca40449a587929","","a93952b550814201a9d0a8fa7fe3d1cf"]},"id":"45e1cbef-eb0d-4d0b-8ab3-526a66fdee7c","outputId":"27043c26-c22f-4004-d022-043145340824"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f0abf80f97842068e51b8792f1ebdd1","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/15.3k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset None/None to /home/qblocks/.cache/huggingface/datasets/hackathon-somos-nlp-2023___parquet/hackathon-somos-nlp-2023--informes_discriminacion_gitana-bd732aeefd500087/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17aa984267e446fa92d50ca1a89f59e0","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df0e4ef3e4b54610bf7c160c412c72a2","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/53.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f51ee9db3c143f1965d14ff63587cf9","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/826k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0929705b82034919bfb3d47ca10c6de7","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d04076d110e45d9abca40449a587929","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating valid split:   0%|          | 0/99 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1791 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset parquet downloaded and prepared to /home/qblocks/.cache/huggingface/datasets/hackathon-somos-nlp-2023___parquet/hackathon-somos-nlp-2023--informes_discriminacion_gitana-bd732aeefd500087/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a93952b550814201a9d0a8fa7fe3d1cf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset = load_dataset('hackathon-somos-nlp-2023/informes_discriminacion_gitana')"]},{"cell_type":"code","execution_count":null,"id":"2ef21ea6-3902-4e3d-982c-03f529fbb64c","metadata":{"id":"2ef21ea6-3902-4e3d-982c-03f529fbb64c","outputId":"e439039a-6ffb-4297-8b82-25d38c3e7587"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sintetico', 'text', 'intervencion', 'tipo_discriminacion', 'resultado'],\n","        num_rows: 1791\n","    })\n","    test: Dataset({\n","        features: ['sintetico', 'text', 'intervencion', 'tipo_discriminacion', 'resultado'],\n","        num_rows: 100\n","    })\n","    valid: Dataset({\n","        features: ['sintetico', 'text', 'intervencion', 'tipo_discriminacion', 'resultado'],\n","        num_rows: 99\n","    })\n","})"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","source":["vacio = {\"hechos\": [],\n","        \"intervencion\": [],\n","        \"resultado\": []}\n","for i, data in enumerate(dataset[\"train\"]):\n","    if len(data[\"text\"]) == 0:\n","        vacio[\"hechos\"].append(i)\n","    if len(data[\"intervencion\"]) == 0:\n","        vacio[\"intervencion\"].append(i)\n","    if len(data[\"resultado\"]) == 0:\n","        vacio[\"resultado\"].append(i)"],"metadata":{"id":"2EdU3YRO3PvI"},"id":"2EdU3YRO3PvI","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"0841e622-0534-4f0a-ae8d-e88442cbdbf4","metadata":{"id":"0841e622-0534-4f0a-ae8d-e88442cbdbf4","outputId":"4e9f0021-1daa-445b-fe28-e630299011c9"},"outputs":[{"data":{"text/plain":["{'hechos': [],\n"," 'intervencion': [119,\n","  209,\n","  311,\n","  565,\n","  851,\n","  869,\n","  1091,\n","  1187,\n","  1354,\n","  1403,\n","  1460,\n","  1491,\n","  1520,\n","  1523,\n","  1625,\n","  1703],\n"," 'resultado': []}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["vacio"]},{"cell_type":"markdown","id":"45a8ae0c-c696-4947-ae2d-aa51250e883c","metadata":{"id":"45a8ae0c-c696-4947-ae2d-aa51250e883c"},"source":["Eliminamos las intervenciones vacías"]},{"cell_type":"code","execution_count":null,"id":"5151a593-4cb3-4039-b84e-15b4a13da50d","metadata":{"id":"5151a593-4cb3-4039-b84e-15b4a13da50d","outputId":"90d54817-5003-4f85-cc92-10c9dfa0376b"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/06/23 12:27:47] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> WARNING:datasets.fingerprint:Parameter <span style=\"color: #008000; text-decoration-color: #008000\">'indices'</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">generator</span><span style=\"color: #000000; text-decoration-color: #000000\"> object </span> <a href=\"file:///home/qblocks/.local/lib/python3.8/site-packages/datasets/fingerprint.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">fingerprint.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/qblocks/.local/lib/python3.8/site-packages/datasets/fingerprint.py#327\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">327</span></a>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">&lt;genexpr&gt; at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7f20d4f02820</span><span style=\"font-weight: bold\">&gt;</span> of the transform                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         datasets.arrow_dataset.Dataset.select couldn't be hashed properly,  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         a random hash was used instead. Make sure your transforms and       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameters are serializable with pickle or dill for the dataset     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         fingerprinting and caching to work. If you reuse this transform,    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         the caching mechanism will consider it to be different from the     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         previous calls and recompute everything. This warning is only       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         showed once. Subsequent hashing failures won't be showed.           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>\n","</pre>\n"],"text/plain":["\u001b[2;36m[04/06/23 12:27:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m WARNING:datasets.fingerprint:Parameter \u001b[32m'indices'\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mgenerator\u001b[0m\u001b[39m object \u001b[0m \u001b]8;id=734194;file:///home/qblocks/.local/lib/python3.8/site-packages/datasets/fingerprint.py\u001b\\\u001b[2mfingerprint.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=824363;file:///home/qblocks/.local/lib/python3.8/site-packages/datasets/fingerprint.py#327\u001b\\\u001b[2m327\u001b[0m\u001b]8;;\u001b\\\n","\u001b[2;36m                    \u001b[0m         \u001b[39m<genexpr> at \u001b[0m\u001b[1;36m0x7f20d4f02820\u001b[0m\u001b[1m>\u001b[0m of the transform                       \u001b[2m                  \u001b[0m\n","\u001b[2;36m                    \u001b[0m         datasets.arrow_dataset.Dataset.select couldn't be hashed properly,  \u001b[2m                  \u001b[0m\n","\u001b[2;36m                    \u001b[0m         a random hash was used instead. Make sure your transforms and       \u001b[2m                  \u001b[0m\n","\u001b[2;36m                    \u001b[0m         parameters are serializable with pickle or dill for the dataset     \u001b[2m                  \u001b[0m\n","\u001b[2;36m                    \u001b[0m         fingerprinting and caching to work. If you reuse this transform,    \u001b[2m                  \u001b[0m\n","\u001b[2;36m                    \u001b[0m         the caching mechanism will consider it to be different from the     \u001b[2m                  \u001b[0m\n","\u001b[2;36m                    \u001b[0m         previous calls and recompute everything. This warning is only       \u001b[2m                  \u001b[0m\n","\u001b[2;36m                    \u001b[0m         showed once. Subsequent hashing failures won't be showed.           \u001b[2m                  \u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["dataset[\"train\"] = dataset[\"train\"].select(\n","    (\n","        i for i in range(len(dataset[\"train\"])) if i not in set(vacio[\"intervencion\"])\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"id":"62cd432c-29c2-460e-a139-1a6e37369db7","metadata":{"id":"62cd432c-29c2-460e-a139-1a6e37369db7","outputId":"53792ec9-ebd6-415b-fda3-f4beaed551d3"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['sintetico', 'text', 'intervencion', 'tipo_discriminacion', 'resultado'],\n","    num_rows: 1775\n","})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["dataset[\"train\"]"]},{"cell_type":"code","execution_count":null,"id":"bed97530-75c2-4b9b-81e4-5836ee259165","metadata":{"colab":{"referenced_widgets":[""]},"id":"bed97530-75c2-4b9b-81e4-5836ee259165","outputId":"852c9842-be18-4fb5-e579-d18180b9e8db"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1775 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/99 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1775 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/99 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1775 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/99 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["datos_positivo = dataset.filter(lambda x: x[\"resultado\"] == \"Positivo.\")\n","datos_negativo = dataset.filter(lambda x: x[\"resultado\"] == \"Negativo.\")\n","datos_neutro = dataset.filter(lambda x: x[\"resultado\"] == \"Neutro.\")"]},{"cell_type":"code","execution_count":null,"id":"72d9d9fb-ae54-45e2-9504-74071d6d3d6f","metadata":{"id":"72d9d9fb-ae54-45e2-9504-74071d6d3d6f","outputId":"9d935e25-e208-45c4-fd43-94ecd4ffe093"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sintetico', 'text', 'intervencion', 'tipo_discriminacion', 'resultado'],\n","        num_rows: 246\n","    })\n","    test: Dataset({\n","        features: ['sintetico', 'text', 'intervencion', 'tipo_discriminacion', 'resultado'],\n","        num_rows: 11\n","    })\n","    valid: Dataset({\n","        features: ['sintetico', 'text', 'intervencion', 'tipo_discriminacion', 'resultado'],\n","        num_rows: 11\n","    })\n","})"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["datos_positivo"]},{"cell_type":"code","execution_count":null,"id":"92a9d90b-2848-4398-bf93-7065bf4cc7b9","metadata":{"id":"92a9d90b-2848-4398-bf93-7065bf4cc7b9"},"outputs":[],"source":["# dividir el subconjunto de no positivo en dos subconjuntos aleatorios del mismo tamaño\n","datos_neg_train = datos_negativo[\"train\"].train_test_split(train_size=len(datos_positivo[\"train\"]))\n","datos_neu_train = datos_neutro[\"train\"].train_test_split(train_size=len(datos_positivo[\"train\"]))\n","\n","datos_neg_test = datos_negativo[\"test\"].train_test_split(train_size=len(datos_positivo[\"test\"]))\n","datos_neu_test = datos_neutro[\"test\"].train_test_split(train_size=len(datos_positivo[\"test\"]))\n","\n","datos_neg_valid = datos_negativo[\"valid\"].train_test_split(train_size=len(datos_positivo[\"valid\"]))\n","datos_neu_valid = datos_neutro[\"valid\"].train_test_split(train_size=len(datos_positivo[\"valid\"]))"]},{"cell_type":"code","execution_count":null,"id":"21f10c76-7e6b-4211-8c25-d0603eaa4c72","metadata":{"id":"21f10c76-7e6b-4211-8c25-d0603eaa4c72","outputId":"8833440f-6cd9-47d3-c4d7-1d5fd1389542"},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['sintetico', 'text', 'intervencion', 'tipo_discriminacion', 'resultado'],\n","     num_rows: 246\n"," }),\n"," Dataset({\n","     features: ['sintetico', 'text', 'intervencion', 'tipo_discriminacion', 'resultado'],\n","     num_rows: 11\n"," }),\n"," Dataset({\n","     features: ['sintetico', 'text', 'intervencion', 'tipo_discriminacion', 'resultado'],\n","     num_rows: 11\n"," }))"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["datos_neg_train[\"train\"], datos_neg_test[\"train\"], datos_neg_valid[\"train\"]\n","datos_neu_train[\"train\"], datos_neu_test[\"train\"], datos_neu_valid[\"train\"]"]},{"cell_type":"markdown","source":["Como el dataset no está balanceado, vamos a tomar el mismo número de muestras positivas, negativas y neutras."],"metadata":{"id":"WO8kz4d13Z0_"},"id":"WO8kz4d13Z0_"},{"cell_type":"code","execution_count":null,"id":"c851ff8a-803f-4ccd-83ae-981aea10d708","metadata":{"id":"c851ff8a-803f-4ccd-83ae-981aea10d708"},"outputs":[],"source":["from datasets import concatenate_datasets\n","\n","datos_neg = DatasetDict({\"train\": datos_neg_train[\"train\"], \"test\": datos_neg_test[\"train\"], \"valid\": datos_neg_valid[\"train\"]})\n","datos_neu = DatasetDict({\"train\": datos_neu_train[\"train\"], \"test\": datos_neu_test[\"train\"], \"valid\": datos_neu_valid[\"train\"]})\n","concatenate_datasets([datos_positivo[\"train\"], datos_neu[\"train\"], datos_neg[\"train\"]])"]},{"cell_type":"code","execution_count":null,"id":"53ad00c2-56fa-4f93-9f31-5a86695c6650","metadata":{"id":"53ad00c2-56fa-4f93-9f31-5a86695c6650"},"outputs":[],"source":["balanced_dataset = DatasetDict({\n","                               \"train\": concatenate_datasets([datos_positivo[\"train\"], datos_neu[\"train\"], datos_neg[\"train\"]]),\n","                               \"test\": concatenate_datasets([datos_positivo[\"test\"], datos_neu[\"test\"], datos_neg[\"test\"]]),\n","                               \"valid\": concatenate_datasets([datos_positivo[\"valid\"], datos_neu[\"valid\"], datos_neg[\"valid\"]])\n","                               })"]},{"cell_type":"code","execution_count":null,"id":"522ca62b-a08d-498b-91a1-a3c06325835c","metadata":{"id":"522ca62b-a08d-498b-91a1-a3c06325835c"},"outputs":[],"source":["balanced_dataset = balanced_dataset.shuffle(seed=42)"]},{"cell_type":"code","execution_count":null,"id":"13535e88-01c7-4219-ab53-e4681a51d951","metadata":{"id":"13535e88-01c7-4219-ab53-e4681a51d951","outputId":"dd39c458-4727-493a-a398-ee225688d530"},"outputs":[{"data":{"text/plain":["{'sintetico': '0',\n"," 'text': 'La Voz de Galicia publicó una noticia sobre una detención policial en un barrio de Lugo. En el cuerpo de la noticia se mencionaba que los delincuentes eran de etnia gitana.',\n"," 'intervencion': 'Se envió carta de queja al medio.',\n"," 'tipo_discriminacion': 'Discriminación directa',\n"," 'resultado': 'Negativo.'}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["balanced_dataset[\"train\"][0]"]},{"cell_type":"code","execution_count":null,"id":"3c3c1c29-083e-41d0-b096-03203f6b5cad","metadata":{"id":"3c3c1c29-083e-41d0-b096-03203f6b5cad"},"outputs":[],"source":["def format_ds(example):\n","  example[\"input_text\"] = \"<SH>\" + example[\"text\"] + \" Intervención: \" + example[\"intervencion\"] + \" Resultado:\" + example[\"resultado\"] + \"<EH>\"\n","  return example"]},{"cell_type":"code","execution_count":null,"id":"30dc573c-bebe-4222-8ffd-1793a8249c95","metadata":{"colab":{"referenced_widgets":[""]},"id":"30dc573c-bebe-4222-8ffd-1793a8249c95","outputId":"4034e385-9da8-4062-88c5-b1489600a9c0"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/738 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/33 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/33 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["balanced_dataset = balanced_dataset.map(format_ds, remove_columns=[\"sintetico\", \"text\", \"intervencion\", \"tipo_discriminacion\", \"resultado\"])"]},{"cell_type":"code","execution_count":null,"id":"1052ddb6-2fa4-4287-b1b7-b7f99fd59561","metadata":{"id":"1052ddb6-2fa4-4287-b1b7-b7f99fd59561","outputId":"fc5c0fd4-38c5-47a2-e6e2-5d2efec84cb1"},"outputs":[{"data":{"text/plain":["{'input_text': '<SH>La Voz de Galicia publicó una noticia sobre una detención policial en un barrio de Lugo. En el cuerpo de la noticia se mencionaba que los delincuentes eran de etnia gitana. Intervención: Se envió carta de queja al medio. Resultado:Negativo.<EH>'}"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["balanced_dataset[\"train\"][0]"]},{"cell_type":"code","execution_count":null,"id":"055b7d29-3d66-4bd9-84ee-9aadb5ff2575","metadata":{"colab":{"referenced_widgets":[""]},"id":"055b7d29-3d66-4bd9-84ee-9aadb5ff2575","outputId":"f95d4657-4cd4-4922-a7f3-d3f97770f98b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/738 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/33 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/33 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["balanced_dataset = balanced_dataset.map(lambda samples: tokenizer(samples[\"input_text\"]), batched=True)"]},{"cell_type":"markdown","id":"7f575899-707c-4c03-a364-56ecf91f4506","metadata":{"id":"7f575899-707c-4c03-a364-56ecf91f4506"},"source":["### Configuración de los parámetros de entrenamiento"]},{"cell_type":"code","execution_count":null,"id":"bccad16a-b926-4e97-b2b9-f8786356186d","metadata":{"id":"bccad16a-b926-4e97-b2b9-f8786356186d"},"outputs":[],"source":["trainer = transformers.Trainer(\n","    model=model, \n","    train_dataset=balanced_dataset['train'],\n","    eval_dataset=balanced_dataset['test'],\n","    args=transformers.TrainingArguments(\n","        per_device_train_batch_size=4, \n","        gradient_accumulation_steps=4,\n","        warmup_steps=100, \n","        max_steps=1000, \n","        learning_rate=2e-4, \n","        fp16=True,\n","        logging_steps=20, \n","        output_dir='outputs',\n","        report_to='tensorboard'\n","    ),\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",")"]},{"cell_type":"code","execution_count":null,"id":"e5616ebf-97e3-45fb-a70d-53981b734892","metadata":{"id":"e5616ebf-97e3-45fb-a70d-53981b734892"},"outputs":[],"source":["model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"]},{"cell_type":"code","execution_count":null,"id":"232484a9-2487-493b-9f3e-3d84ad4fd903","metadata":{"id":"232484a9-2487-493b-9f3e-3d84ad4fd903","outputId":"b1d06195-58b2-4767-f331-ebf4ecc88814"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/qblocks/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1000/1000 2:33:20, Epoch 21/22]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>20</td>\n","      <td>1.664800</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.548200</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.319800</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.266100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.217100</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>1.178800</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>1.189100</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>1.116100</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>1.111500</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.065000</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>1.052600</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>1.017900</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.987600</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.984300</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.899500</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.916000</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.843500</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.828800</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.799000</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.757100</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.736800</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.655200</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.682300</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.612900</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.585800</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>0.541800</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.518500</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>0.509000</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>0.449100</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.451000</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>0.385600</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>0.400900</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>0.358600</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>0.327700</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.346700</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>0.302600</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>0.297200</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.264700</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>0.261300</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.251500</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>0.245200</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>0.225000</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>0.201500</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>0.220100</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.194100</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>0.190700</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>0.187400</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>0.178500</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>0.186000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.163900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"]},{"data":{"text/plain":["TrainOutput(global_step=1000, training_loss=0.6538836755752564, metrics={'train_runtime': 9209.8727, 'train_samples_per_second': 1.737, 'train_steps_per_second': 0.109, 'total_flos': 2.6633005587904467e+17, 'train_loss': 0.6538836755752564, 'epoch': 21.62})"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"e6b7238f-86cc-4793-85e4-f4aec30bfd55","metadata":{"id":"e6b7238f-86cc-4793-85e4-f4aec30bfd55","outputId":"f34f1e0a-84bc-4114-ce78-8710470ed34e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Perplexity: 9.20\n"]}],"source":["import math\n","\n","eval_results = trainer.evaluate()\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"markdown","id":"b5571a0e-6ee9-47d7-b42f-7e729d456bf8","metadata":{"id":"b5571a0e-6ee9-47d7-b42f-7e729d456bf8"},"source":["La medida de perplexity (perplejidad en español) se utiliza en el campo del procesamiento del lenguaje natural para evaluar la calidad de un modelo de lenguaje. Básicamente, la perplexity se refiere a cuán bien un modelo de lenguaje puede predecir una secuencia de palabras desconocidas, en función de su experiencia previa.\n","\n","En términos generales, cuanto más bajo sea el valor de la perplexity, mejor será el modelo de lenguaje. Un modelo con una perplexity baja tiene una mejor capacidad para predecir la probabilidad de una secuencia de palabras desconocidas. Por otro lado, un modelo con una perplexity alta indica que tiene dificultades para hacer predicciones precisas.\n","\n","Nuestro modelo tiene un valor de perplexity de 9.2"]},{"cell_type":"markdown","id":"4e70a49a-d0ff-47e6-9e0a-4cee47b4eea8","metadata":{"id":"4e70a49a-d0ff-47e6-9e0a-4cee47b4eea8"},"source":["### Subida del modelo al hub"]},{"cell_type":"code","execution_count":null,"id":"42237108-2eb6-4126-93ad-3dfc987ea3c8","metadata":{"id":"42237108-2eb6-4126-93ad-3dfc987ea3c8"},"outputs":[],"source":["model.push_to_hub(\"rwheel/discriminacion_gitana_intervenciones_balanceado\", use_auth_token=True)\n","\n","tokenizer.push_to_hub(\"rwheel/discriminacion_gitana_intervenciones_balanceado\", use_auth_token=True)"]},{"cell_type":"markdown","id":"5599a46e-9877-410b-bde1-09a5100abb72","metadata":{"id":"5599a46e-9877-410b-bde1-09a5100abb72"},"source":["## Cargar el adapter del Hub"]},{"cell_type":"code","execution_count":null,"id":"47fd936d-7050-44be-b047-ed18d3ef7850","metadata":{"id":"47fd936d-7050-44be-b047-ed18d3ef7850"},"outputs":[],"source":["import torch\n","from peft import PeftModel, PeftConfig\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","peft_model_id = \"rwheel/discriminacion_gitana_intervenciones\"\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n","tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n","# Load the Lora model\n","model = PeftModel.from_pretrained(model, peft_model_id)"]},{"cell_type":"code","execution_count":null,"id":"35c75008-507e-48c7-b661-fcae46a07d9b","metadata":{"id":"35c75008-507e-48c7-b661-fcae46a07d9b"},"outputs":[],"source":["model.config.use_cache = False"]},{"cell_type":"code","execution_count":null,"id":"9638d901-51f7-4f56-86d6-01e4b35a0ec8","metadata":{"id":"9638d901-51f7-4f56-86d6-01e4b35a0ec8"},"outputs":[],"source":["def predecir_intervencion(text):\n","  text = \"<SH>\" + text + \" Intervención: \"\n","  batch = tokenizer(text, return_tensors='pt')\n","  with torch.cuda.amp.autocast():\n","    output_tokens = model.generate(**batch, max_new_tokens=256, eos_token_id=50258)\n","\n","  return tokenizer.decode(output_tokens[0], skip_special_tokens=False)"]},{"cell_type":"code","execution_count":null,"id":"512eddd4-ba50-4f8f-a66c-8a0a1539153a","metadata":{"colab":{"referenced_widgets":["7b3913af006a41c2a88406c6009b72d1"]},"id":"512eddd4-ba50-4f8f-a66c-8a0a1539153a","outputId":"40a2becf-0aff-4770-ded2-5fb6c19d4a1c"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/06/23 15:04:38] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> WARNING:datasets.builder:Found cached dataset parquet                   <a href=\"file:///home/qblocks/.local/lib/python3.8/site-packages/datasets/builder.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">builder.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/qblocks/.local/lib/python3.8/site-packages/datasets/builder.py#817\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">817</span></a>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080\">/home/qblocks/.cache/huggingface/datasets/rwheel___parquet/rwheel--inf</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">ormes_discriminacion_gitana-7a3ddd9628f402a3/0.0.0/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">2a3b91fbd88a2c90d1db</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n","<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">bb32b460cf621d31bd5b05b934492fdef7d8d6f236ec</span><span style=\"font-weight: bold\">)</span>                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n","</pre>\n"],"text/plain":["\u001b[2;36m[04/06/23 15:04:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m WARNING:datasets.builder:Found cached dataset parquet                   \u001b]8;id=234053;file:///home/qblocks/.local/lib/python3.8/site-packages/datasets/builder.py\u001b\\\u001b[2mbuilder.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=146316;file:///home/qblocks/.local/lib/python3.8/site-packages/datasets/builder.py#817\u001b\\\u001b[2m817\u001b[0m\u001b]8;;\u001b\\\n","\u001b[2;36m                    \u001b[0m         \u001b[1m(\u001b[0m\u001b[35m/home/qblocks/.cache/huggingface/datasets/rwheel___parquet/rwheel--inf\u001b[0m \u001b[2m              \u001b[0m\n","\u001b[2;36m                    \u001b[0m         \u001b[35mormes_discriminacion_gitana-7a3ddd9628f402a3/0.0.0/\u001b[0m\u001b[95m2a3b91fbd88a2c90d1db\u001b[0m \u001b[2m              \u001b[0m\n","\u001b[2;36m                    \u001b[0m         \u001b[95mbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\u001b[0m\u001b[1m)\u001b[0m                           \u001b[2m              \u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b3913af006a41c2a88406c6009b72d1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","ldataset = load_dataset('rwheel/informes_discriminacion_gitana')"]},{"cell_type":"code","execution_count":null,"id":"dcbc1e33-504b-4d01-b2e1-887e57b00bf3","metadata":{"id":"dcbc1e33-504b-4d01-b2e1-887e57b00bf3","outputId":"ffaab7fd-6df4-430b-d7c0-ac3e394c1def"},"outputs":[{"data":{"text/plain":["'Una familia gitana que reside en Mérida decide viajar a visitar a un familiar que está enfermo en Zalamea de la Serena. Durante el trayecto, hacen una parada para preguntar por qué carretera continuar cuando de pronto son parados por dos furgonetas de la guardia civil. Nos informan de que fueron cacheados todos los miembros de la familia, incluso los jóvenes que viajaban con ellos. Cuando la guardia civil comprobó por sus propios testimonios que iban a ver a un familiar que estaba enfermo, les dejo seguir su viaje. A pesar de pedir una explicación de por qué los cachearon los guardia civiles no les comentaron nada. Nos trasladan que se sintieron muy mal, porque en cuanto la guardia civil se dio cuenta que eran gitanos, los pararon. Era de noche y por miedo, no identificaron ni los vehículos ni a los agentes.'"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["ldataset[\"valid\"][9][\"text\"]"]},{"cell_type":"code","execution_count":null,"id":"7564abec-8fbb-4e3b-8eaf-a79b9c084033","metadata":{"id":"7564abec-8fbb-4e3b-8eaf-a79b9c084033","outputId":"f9a4ae78-0e1e-4edb-bd2f-f603ef413502"},"outputs":[{"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_3240/536106888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#hechos = ldataset[\"valid\"][9][\"text\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhechos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Una joven gitana fue a hacer la compra acompañada de su hija de 12 años y su sobrina de ocho. Mientras compraban, el vigilante de seguridad no dejó de seguirlas en todo momento.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredecir_intervencion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhechos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_3240/2459434312.py\u001b[0m in \u001b[0;36mpredecir_intervencion\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredecir_intervencion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"<SH>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Intervención: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50258\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}],"source":["hechos = ldataset[\"valid\"][9][\"text\"]\n","output = predecir_intervencion(hechos)"]},{"cell_type":"markdown","id":"04fa3a94-78c2-4c48-85dc-2c93dc4a6920","metadata":{"id":"04fa3a94-78c2-4c48-85dc-2c93dc4a6920"},"source":["### Interfaz con gradio"]},{"cell_type":"code","execution_count":null,"id":"c360526c-0c7c-42b1-809d-ce60332183c5","metadata":{"id":"c360526c-0c7c-42b1-809d-ce60332183c5"},"outputs":[],"source":["! pip install gradio"]},{"cell_type":"code","execution_count":null,"id":"74be59e7-731f-4859-abfc-9f4d8b0cf6c1","metadata":{"id":"74be59e7-731f-4859-abfc-9f4d8b0cf6c1","outputId":"d0bfa7ed-6210-4ecd-e74b-39011fd68473"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-04-07 09:37:22.932330: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"]},{"name":"stdout","output_type":"stream","text":["\n","===================================BUG REPORT===================================\n","Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n","================================================================================\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n","CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n","CUDA SETUP: Detected CUDA version 110\n","CUDA SETUP: Loading binary /home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda110.so...\n"]},{"name":"stderr","output_type":"stream","text":["/home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n","  warn(msg)\n","Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"]},{"name":"stdout","output_type":"stream","text":["Running on local URL:  http://127.0.0.1:7860\n","Running on public URL: https://4aeb53c44685dcd858.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"]},{"data":{"text/html":["<div><iframe src=\"https://4aeb53c44685dcd858.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":4,"metadata":{},"output_type":"execute_result"},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","/home/qblocks/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n","  warnings.warn(\n","/home/qblocks/.local/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","Traceback (most recent call last):\n","  File \"/home/qblocks/.local/lib/python3.8/site-packages/gradio/routes.py\", line 393, in run_predict\n","    output = await app.get_blocks().process_api(\n","  File \"/home/qblocks/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1108, in process_api\n","    result = await self.call_function(\n","  File \"/home/qblocks/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 915, in call_function\n","    prediction = await anyio.to_thread.run_sync(\n","  File \"/home/qblocks/.local/lib/python3.8/site-packages/anyio/to_thread.py\", line 31, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/home/qblocks/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/home/qblocks/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/tmp/ipykernel_3240/2309402459.py\", line 23, in predecir_intervencion\n","    resultado = aux.split(\"Resultado:\")[1].split(\"<EH>\")[0].strip()\n","IndexError: list index out of range\n","Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]}],"source":["import gradio as gr\n","import torch\n","from peft import PeftModel, PeftConfig\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","peft_model_id = \"hackathon-somos-nlp-2023/discriminacion_gitana_intervenciones_balanceado\"\n","config = PeftConfig.from_pretrained(peft_model_id)\n","model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n","tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n","# Load the Lora model\n","model = PeftModel.from_pretrained(model, peft_model_id)\n","\n","def predecir_intervencion(text):\n","    text = \"<SH>\" + text + \" Intervención: \"\n","    batch = tokenizer(text, return_tensors='pt')\n","    with torch.cuda.amp.autocast():\n","        output_tokens = model.generate(**batch, max_new_tokens=256, eos_token_id=50258)\n","\n","    output = tokenizer.decode(output_tokens[0], skip_special_tokens=False)\n","\n","    aux = output.split(\"Intervención:\")[1].strip()\n","    intervencion = aux.split(\"Resultado:\")[0].strip()\n","    resultado = aux.split(\"Resultado:\")[1].split(\"<EH>\")[0].strip()\n","    \n","    return intervencion, resultado\n","\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"Predicción de intervenciones para mitigar el daño racista en el pueblo gitano\")\n","    with gr.Row():\n","        hechos = gr.Textbox(placeholder=\"Un alumno gitano de un Instituto...\", label=\"Hechos\")\n","    with gr.Row():\n","        intervencion = gr.Textbox(label=\"Intervención\")\n","        resultado = gr.Textbox(label=\"Resultado\")\n","        \n","    btn = gr.Button(\"Go\")\n","    btn.click(fn=predecir_intervencion, inputs=hechos, outputs=[intervencion, resultado])\n","\n","demo.launch(share=True)"]},{"cell_type":"code","execution_count":null,"id":"5cacaccf-4e45-41f8-b3c0-ee9db5a266cd","metadata":{"id":"5cacaccf-4e45-41f8-b3c0-ee9db5a266cd"},"outputs":[],"source":["Una joven gitana fue a hacer la compra acompañada de su hija de 12 años y su sobrina de ocho. Mientras compraban, el vigilante de seguridad no dejó de seguirlas en todo momento."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}